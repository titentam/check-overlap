import io
import json
import pandas as pd
import streamlit as st
from datetime import datetime

st.set_page_config(page_title="Excel Merge Tool", layout="wide")

st.title("üîó Excel Merge & Enrichment Tool")
st.write(
    """
T·∫£i l√™n 2 file Excel c√≥ c√πng header:

- File 1: Nhi·ªÅu record h∆°n nh∆∞ng thi·∫øu d·ªØ li·ªáu.
- File 2: √çt record h∆°n nh∆∞ng d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß h∆°n.

Ch·ªçn c√°c c·ªôt kh√≥a ƒë·ªÉ ƒë·ªëi chi·∫øu. Tool s·∫Ω:
1. Gh√©p (merge) theo c√°c c·ªôt kh√≥a.
2. V·ªõi c√°c √¥ b·ªã thi·∫øu trong File 1 s·∫Ω ƒë∆∞·ª£c b·ªï sung b·∫±ng d·ªØ li·ªáu t·ª´ File 2 n·∫øu c√≥.
3. Ghi ƒë√® c√°c √¥ r·ªóng ho·∫∑c gi√° tr·ªã null trong File 1 b·∫±ng gi√° tr·ªã t∆∞∆°ng ·ª©ng ·ªü File 2.
4. Th√™m c√°c record c√≤n thi·∫øu (c√≥ trong File 2 nh∆∞ng kh√¥ng c√≥ trong File 1).

Sau khi x·ª≠ l√Ω, c√≥ th·ªÉ t·∫£i v·ªÅ file k·∫øt qu·∫£.
"""
)


@st.cache_data(show_spinner=False)
def load_excel(file_bytes: bytes) -> pd.DataFrame:
    return pd.read_excel(io.BytesIO(file_bytes))


with st.sidebar:
    st.header("üì§ Upload Files")
    f1 = st.file_uploader("File 1 (thi·∫øu d·ªØ li·ªáu)", type=["xls", "xlsx"], key="file1")
    f2 = st.file_uploader("File 2 (ƒë·∫ßy ƒë·ªß d·ªØ li·ªáu)", type=["xls", "xlsx"], key="file2")

    merge_mode = st.selectbox(
        "Ch·∫ø ƒë·ªô b·ªï sung",
        [
            "Ch·ªâ ƒëi·ªÅn v√†o √¥ tr·ªëng ·ªü File 1",
            "Ghi ƒë√® n·∫øu kh√°c (File 2 ∆∞u ti√™n)",
        ],
    )

    add_missing_rows = st.checkbox("Th√™m record m·ªõi t·ª´ File 2 v√†o File 1", value=True)

    st.markdown("---")
    st.caption("¬© 2025 Merge Tool")

if f1 and f2:
    try:
        df1 = load_excel(f1.getvalue())
        df2 = load_excel(f2.getvalue())
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc file: {e}")
        st.stop()

    if df1.empty or df2.empty:
        st.warning("M·ªôt trong hai file kh√¥ng c√≥ d·ªØ li·ªáu.")
        st.stop()

    st.subheader("üëÅÔ∏è Xem tr∆∞·ªõc d·ªØ li·ªáu")
    with st.expander("File 1 - Thi·∫øu d·ªØ li·ªáu", expanded=False):
        st.dataframe(df1.head(200))
    with st.expander("File 2 - ƒê·∫ßy ƒë·ªß d·ªØ li·ªáu", expanded=False):
        st.dataframe(df2.head(200))

    common_columns = list(set(df1.columns).intersection(df2.columns))
    if not common_columns:
        st.error("Hai file kh√¥ng c√≥ c·ªôt chung n√†o ƒë·ªÉ x·ª≠ l√Ω.")
        st.stop()

    st.subheader("üîë Ch·ªçn c·ªôt kh√≥a")
    key_cols = st.multiselect(
        "Ch·ªçn c√°c c·ªôt d√πng l√†m kh√≥a (n√™n ch·ªçn ƒë·ªß ƒë·ªÉ x√°c ƒë·ªãnh duy nh·∫•t 1 record)",
        options=common_columns,
    )

    if key_cols:
        if not all(col in df1.columns for col in key_cols) or not all(
            col in df2.columns for col in key_cols
        ):
            st.error("C·ªôt kh√≥a kh√¥ng t·ªìn t·∫°i trong c·∫£ hai file.")
            st.stop()

        # Chu·∫©n h√≥a kh√≥a (strip + lower cho text) ƒë·ªÉ tƒÉng kh·∫£ nƒÉng kh·ªõp
        def normalize_key_cols(df: pd.DataFrame, cols):
            norm_df = df.copy()
            for c in cols:
                if pd.api.types.is_object_dtype(norm_df[c]):
                    norm_df[c] = norm_df[c].astype(str).str.strip().str.lower()
            return norm_df

        norm_df1 = normalize_key_cols(df1, key_cols)
        norm_df2 = normalize_key_cols(df2, key_cols)
        key_name = "__merge_key__"
        norm_df1[key_name] = norm_df1[key_cols].astype(str).agg("|".join, axis=1)
        norm_df2[key_name] = norm_df2[key_cols].astype(str).agg("|".join, axis=1)

        # Map t·ª´ key chu·∫©n h√≥a -> index g·ªëc ƒë·ªÉ c·∫≠p nh·∫≠t df1 sau khi merge
        key_to_index_df1 = norm_df1[key_name].to_dict()

        st.info(f"S·ªë b·∫£n ghi File 1: {len(df1)} | File 2: {len(df2)}")

        # ================== MAPPING C·ªòT TH·ª¶ C√îNG ==================
        st.subheader("üß¨ Mapping c·ªôt c·∫≠p nh·∫≠t / ghi ƒë√®")
        st.caption(
            "B·∫°n c√≥ th·ªÉ ch·ªçn c·ªôt ·ªü File 1 (ƒë√≠ch) v√† x√°c ƒë·ªãnh c·ªôt t∆∞∆°ng ·ª©ng ·ªü File 2 (ngu·ªìn) ƒë·ªÉ b·ªï sung ho·∫∑c ghi ƒë√®. V√≠ d·ª•: 'h·ªç t√™n' (File 2) ‚ûù 'H·ªç v√† t√™n' (File 1). N·∫øu kh√¥ng b·∫≠t, tool ch·ªâ x·ª≠ l√Ω c√°c c·ªôt tr√πng t√™n (lo·∫°i tr·ª´ c·ªôt kh√≥a). C√≥ th·ªÉ l∆∞u / t·∫£i l·∫°i c·∫•u h√¨nh mapping d∆∞·ªõi d·∫°ng JSON."
        )
        manual_mapping_enabled = st.checkbox(
            "B·∫≠t t√πy ch·ªânh mapping c·ªôt kh√°c t√™n (File 2 ‚ûù File 1)", value=False
        )
        # Upload file mapping JSON (√°p d·ª•ng tr∆∞·ªõc khi render controls)
        mapping_upload = st.file_uploader(
            "T·∫£i file mapping (.json) n·∫øu c√≥", type=["json"], key="mapping_upload"
        )

        # C√°c c·ªôt ·ª©ng vi√™n (lo·∫°i b·ªè c·ªôt kh√≥a)
        common_non_key_cols = [c for c in common_columns if c not in key_cols]
        dest_candidates = [c for c in df1.columns if c not in key_cols]
        source_candidates = [c for c in df2.columns if c not in key_cols]

        mapping: dict[str, str] = {}
        loaded_mapping = None
        if mapping_upload is not None:
            try:
                raw_txt = mapping_upload.getvalue().decode("utf-8")
                loaded_json = json.loads(raw_txt)
                if isinstance(loaded_json, list):
                    # Expect list of {"dest":..., "src":...}
                    loaded_mapping = {
                        item["dest"]: item["src"]
                        for item in loaded_json
                        if isinstance(item, dict) and "dest" in item and "src" in item
                    }
                elif isinstance(loaded_json, dict):
                    loaded_mapping = {str(k): str(v) for k, v in loaded_json.items()}
                else:
                    raise ValueError(
                        "ƒê·ªãnh d·∫°ng JSON kh√¥ng h·ª£p l·ªá (ch·ªâ h·ªó tr·ª£ object ho·∫∑c list)."
                    )
                # L·ªçc h·ª£p l·ªá
                loaded_mapping = {
                    d: s
                    for d, s in loaded_mapping.items()
                    if d in dest_candidates and s in source_candidates
                }
                if not loaded_mapping:
                    st.warning(
                        "File mapping JSON kh√¥ng c√≥ c·∫∑p h·ª£p l·ªá v·ªõi d·ªØ li·ªáu hi·ªán t·∫°i."
                    )
                else:
                    # Set session state ƒë·ªÉ auto ch·ªçn
                    st.session_state["dest_selected_cols"] = list(loaded_mapping.keys())
                    for d, s in loaded_mapping.items():
                        st.session_state[f"map_src_for_{d}"] = s
                    st.success(f"ƒê√£ n·∫°p mapping JSON: {len(loaded_mapping)} c·∫∑p.")
            except Exception as e:
                st.error(f"L·ªói ƒë·ªçc mapping JSON: {e}")

        if manual_mapping_enabled:
            # N·∫øu ƒë√£ upload mapping th√¨ d√πng n√≥ l√†m default; n·∫øu kh√¥ng th√¨ d√πng m·∫∑c ƒë·ªãnh c·ªôt tr√πng t√™n c√≥ trong c·∫£ hai
            default_dest = (
                list(loaded_mapping.keys())
                if loaded_mapping
                else [c for c in dest_candidates if c in source_candidates]
            )
            dest_selected = st.multiselect(
                "Ch·ªçn c√°c c·ªôt ƒë√≠ch ·ªü File 1 c·∫ßn c·∫≠p nh·∫≠t (c·ªôt nh·∫≠n d·ªØ li·ªáu)",
                options=dest_candidates,
                default=default_dest,
                key="dest_selected_cols",
            )
            if dest_selected:
                st.markdown("**Ch·ªçn c·ªôt ngu·ªìn t∆∞∆°ng ·ª©ng t·ª´ File 2 (ghi ƒë√® ‚ûù ƒë√≠ch):**")
                for dest in dest_selected:
                    # N·∫øu c√≥ mapping load s·∫µn => ∆∞u ti√™n
                    default_src = (
                        (loaded_mapping.get(dest) if loaded_mapping else None)
                        or (dest if dest in source_candidates else None)
                        or (source_candidates[0] if source_candidates else None)
                    )
                    if not source_candidates:
                        st.error("File 2 kh√¥ng c√≥ c·ªôt n√†o ƒë·ªÉ mapping.")
                        break
                    src = st.selectbox(
                        f"Ngu·ªìn cho '{dest}'",
                        options=source_candidates,
                        index=(
                            source_candidates.index(default_src)
                            if default_src in source_candidates
                            else 0
                        ),
                        key=f"map_src_for_{dest}",
                    )
                    mapping[dest] = src
                if mapping:
                    mapping_preview = pd.DataFrame(
                        [
                            {"C·ªôt File 1 (ƒë√≠ch)": d, "C·ªôt File 2 (ngu·ªìn)": s}
                            for d, s in mapping.items()
                        ]
                    )
                    st.dataframe(mapping_preview, use_container_width=True)
                    # Download mapping JSON
                    mapping_json_str = json.dumps(mapping, ensure_ascii=False, indent=2)
                    st.download_button(
                        "üíæ T·∫£i mapping JSON",
                        data=mapping_json_str.encode("utf-8"),
                        file_name="mapping_config.json",
                        mime="application/json",
                    )
            else:
                st.info("Ch∆∞a ch·ªçn c·ªôt n√†o ƒë·ªÉ mapping.")
        else:
            mapping = {c: c for c in common_non_key_cols}
            # Cho ph√©p t·∫£i mapping m·∫∑c ƒë·ªãnh
            with st.expander("Xem / t·∫£i mapping m·∫∑c ƒë·ªãnh (tr√πng t√™n)", expanded=False):
                if mapping:
                    mapping_preview = pd.DataFrame(
                        [
                            {"C·ªôt File 1 (ƒë√≠ch)": d, "C·ªôt File 2 (ngu·ªìn)": s}
                            for d, s in mapping.items()
                        ]
                    )
                    st.dataframe(mapping_preview, use_container_width=True)
                    mapping_json_str = json.dumps(mapping, ensure_ascii=False, indent=2)
                    st.download_button(
                        "üíæ T·∫£i mapping m·∫∑c ƒë·ªãnh JSON",
                        data=mapping_json_str.encode("utf-8"),
                        file_name="mapping_default.json",
                        mime="application/json",
                    )

        if st.button("üöÄ Th·ª±c hi·ªán merge", type="primary"):
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                df1_merged = df1.copy()

                # Th√™m c·ªôt key v√†o b·∫£n g·ªëc File 2 ƒë·ªÉ tra c·ª©u tr·ª±c ti·∫øp kh√¥ng b·ªã l·ªách index
                df2_with_key = df2.copy()
                df2_with_key[key_name] = norm_df2[key_name]
                df2_indexed = df2_with_key.set_index(key_name)

                norm_df1_keys = norm_df1[key_name].tolist()
                existing_keys_set = set(norm_df1_keys)

                # C·∫£nh b√°o kh√≥a tr√πng
                dup1 = norm_df1[norm_df1.duplicated(key_name, keep=False)][
                    key_name
                ].unique()
                dup2 = norm_df2[norm_df2.duplicated(key_name, keep=False)][
                    key_name
                ].unique()
                if len(dup1) > 0:
                    st.warning(
                        f"File 1 c√≥ {len(dup1)} kh√≥a tr√πng (s·∫Ω c·∫≠p nh·∫≠t tu·∫ßn t·ª±, b·∫£n ghi xu·∫•t hi·ªán sau c√≥ th·ªÉ ghi ƒë√® k·∫øt qu·∫£ tr∆∞·ªõc)."
                    )
                if len(dup2) > 0:
                    st.warning(
                        f"File 2 c√≥ {len(dup2)} kh√≥a tr√πng (gi·ªØ b·∫£n ghi cu·ªëi c√πng cho m·ªói kh√≥a)."
                    )
                    df2_indexed = df2_indexed[
                        ~df2_indexed.index.duplicated(keep="last")
                    ]

                updated_count = 0
                filled_cells = 0
                overwritten_cells = 0
                added_rows = 0
                if manual_mapping_enabled and not mapping:
                    st.error("Kh√¥ng c√≥ mapping h·ª£p l·ªá ƒë·ªÉ th·ª±c hi·ªán c·∫≠p nh·∫≠t.")
                    st.stop()

                mapping_pairs = list(mapping.items())  # (dest_col, src_col)

                for row_idx, key in enumerate(norm_df1_keys):
                    if key in df2_indexed.index:
                        row2 = df2_indexed.loc[key]
                        any_updated = False
                        for dest_col, src_col in mapping_pairs:
                            if (
                                dest_col not in df1_merged.columns
                                or src_col not in row2.index
                            ):
                                continue
                            val1 = df1_merged.at[row_idx, dest_col]
                            val2 = row2[src_col]
                            if pd.isna(val2) or (
                                isinstance(val2, str) and val2.strip() == ""
                            ):
                                continue
                            if merge_mode == "Ch·ªâ ƒëi·ªÅn v√†o √¥ tr·ªëng ·ªü File 1":
                                if pd.isna(val1) or val1 == "":
                                    df1_merged.at[row_idx, dest_col] = val2
                                    filled_cells += 1
                                    any_updated = True
                            else:  # Ghi ƒë√® n·∫øu kh√°c
                                if pd.isna(val1) or val1 == "":
                                    df1_merged.at[row_idx, dest_col] = val2
                                    filled_cells += 1
                                    any_updated = True
                                elif val1 != val2:
                                    df1_merged.at[row_idx, dest_col] = val2
                                    overwritten_cells += 1
                                    any_updated = True
                        if any_updated:
                            updated_count += 1

                if add_missing_rows:
                    missing_keys = [
                        k for k in df2_indexed.index if k not in existing_keys_set
                    ]
                    if missing_keys:
                        rows_to_append = df2_indexed.loc[missing_keys].copy()
                        # B·ªè key k·ªπ thu·∫≠t tr∆∞·ªõc khi append
                        if key_name in rows_to_append.columns:
                            rows_to_append = rows_to_append.drop(columns=[key_name])
                        # N·∫øu mapping th·ªß c√¥ng: ƒë·∫£m b·∫£o c√°c c·ªôt ƒë√≠ch l·∫•y d·ªØ li·ªáu t·ª´ ngu·ªìn t∆∞∆°ng ·ª©ng
                        if mapping:
                            for dest_col, src_col in mapping.items():
                                if (
                                    dest_col in df1_merged.columns
                                    and src_col in rows_to_append.columns
                                ):
                                    rows_to_append[dest_col] = rows_to_append[src_col]
                        df1_merged = pd.concat(
                            [df1_merged, rows_to_append], ignore_index=True
                        )
                        added_rows = len(rows_to_append)

                # Xu·∫•t file k·∫øt qu·∫£
                out_buffer = io.BytesIO()
                output_filename = (
                    f"merged_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                )
                with pd.ExcelWriter(out_buffer, engine="openpyxl") as writer:
                    df1_merged.to_excel(writer, index=False, sheet_name="Merged")

                st.success("Ho√†n th√†nh!")
                st.write(
                    f"C·∫≠p nh·∫≠t t·ª´ File 2: {updated_count} record | ƒêi·ªÅn √¥ tr·ªëng: {filled_cells} | Ghi ƒë√®: {overwritten_cells} | Th√™m m·ªõi: {added_rows}"
                )
                st.download_button(
                    "‚¨áÔ∏è T·∫£i file k·∫øt qu·∫£",
                    data=out_buffer.getvalue(),
                    file_name=output_filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
    else:
        st.info("H√£y ch·ªçn √≠t nh·∫•t 1 c·ªôt kh√≥a ƒë·ªÉ ti·∫øp t·ª•c.")
else:
    st.warning("H√£y upload c·∫£ 2 file Excel ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

st.markdown("---")
st.caption("Developed with ‚ù§Ô∏è using Streamlit & pandas")
